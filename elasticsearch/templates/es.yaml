{{- if .Capabilities.APIVersions.Has "rbac.authorization.k8s.io/v1beta1"}}

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: {{.Values.name}}
  labels:
    app: {{.Values.name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups: [""]
  resources: ["services", "namespaces", "endpoints"]
  verbs: ["get"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: {{.Values.name}}
  namespace: {{.Values.namespace}}
  labels:
    app: {{.Values.name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: {{.Values.name}}
  namespace: {{.Values.namespace}}
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: {{.Values.name}}
  apiGroup: ""

{{- end}}

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{.Values.name}}
  namespace: {{.Values.namespace}}
  labels:
    app: {{.Values.name}}
    version: v1
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: v1
kind: Service
metadata:
  name: {{.Values.name}}
  namespace: {{.Values.namespace}}
  labels:
    app: {{.Values.name}}
spec:
  ports:
  - port: {{.Values.port}}
    name: {{.Values.port_name}}
  clusterIP: None
  selector:
    name: {{.Values.port_selector}}
---
apiVersion: v1
kind: Service
metadata:
  name: {{.Values.cluster_name}}
  namespace: {{.Values.namespace}}
  labels:
    app: {{.Values.cluster_name}}
spec:
  ports:
  - port: {{.Values.cluster_port}}
    name: {{.Values.cluster_port_name}}
  clusterIP: None
  selector:
    app: {{.Values.name}}
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{.Values.master_name}}
  namespace: {{.Values.namespace}}
spec:
  serviceName: "{{.Values.master_name}}"
  replicas: {{.Values.master_replicas}}
  template:
    metadata:
      labels:
        app: {{.Values.name}}
        name: {{.Values.master_name}}
      annotations:
        pod.alpha.kubernetes.io/init-containers: '[
            {
                "name": "max-map-count-set",
                "image": "quay.io/samsung_cnct/set_max_map_count:1.1",
                "volumeMounts": [
                    {
                        "name": "hostproc",
                        "mountPath": "/hostproc"
                    }
                ]
            }]'
    spec:
      serviceAccount: {{.Values.name}}
      serviceAccountName: {{.Values.name}}
      containers:
      - name: {{.Values.name}}
        image: {{.Values.image}}
        ports:
        - name: {{.Values.cluster_port_name}}
          containerPort: {{.Values.cluster_port}}
        #  resource notes:  master is light weight so half a CPU.  Less then 4GB causes the container to OOM
        resources:
          limits:
            cpu: {{.Values.master_cpu_limits}}
            memory: {{.Values.master_memory_limits}}
          requests:
            cpu: {{.Values.master_cpu_requests}}
            memory: {{.Values.master_memory_requests}}
        env:
        - name: CLUSTER_NAME
          value: "{{.Values.cluster_name}}"
        - name: NODE_DATA
          value: "false"
        - name: NODE_MASTER
          value: "true"
        - name: SERVICE
          value: "{{.Values.cluster_name}}"
        #  the min/max *must* be the same due to elasticsearch 5.0 bootstrap checks
        - name: ES_JAVA_OPTS
          value: "-Xms{{ .Values.master_Xms_Xmx }} -Xmx{{ .Values.master_Xms_Xmx }} -Djava.net.preferIPv4Stack=true"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        imagePullPolicy: {{.Values.master_imagePullPolicy}}
      {{- if .Values.persistence.enabled }}
        volumeMounts:
        - name: esdata
          mountPath: /usr/share/elasticsearch/data
      {{- end }}
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
      {{- if .Values.master_tolerations }}
      tolerations:
      {{- range .Values.master_tolerations }}
      - key: {{ .key | default ("") }}
        value: {{ .value | default ("")}}
        operator: {{ .operator | default ("Equal") }}
        effect: {{ .effect  | default ("")}}
      {{- end }}
      {{- end }}
      {{- if .Values.master_scheduling.affinity }}
      {{- if .Values.master_scheduling.affinity.node }}
      {{- if .Values.master_scheduling.affinity.node.labels }}
      affinity:
        nodeAffinity:
          {{ .Values.master_scheduling.affinity.node.type }}:
            nodeSelectorTerms:
            - matchExpressions:
                {{- range .Values.master_scheduling.affinity.node.labels }}
                - key: {{ .key }}
                  operator: {{ .operator }}
                  values:
                  {{- range .values }}
                  - {{ . }}
                  {{- end }}
                {{- end }}
      {{- end }}
      {{- end }}
      {{- end }}
  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: esdata
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {{.Values.master_volume_storage}}
  {{- end }}

---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{.Values.data_name}}
  namespace: {{.Values.namespace}}
spec:
  serviceName: "{{.Values.data_name}}"
  replicas: {{.Values.data_replicas}}
  template:
    metadata:
      labels:
        app: {{.Values.name}}
        name: {{.Values.data_name}}
      annotations:
        pod.alpha.kubernetes.io/init-containers: '[
            {
                "name": "max-map-count-set",
                "image": "quay.io/samsung_cnct/set_max_map_count:1.1",
                "volumeMounts": [
                    {
                        "name": "hostproc",
                        "mountPath": "/hostproc"
                    }
                ]
            }]'
    spec:
      serviceAccount: {{.Values.name}}
      serviceAccountName: {{.Values.name}}
      containers:
      - name: {{.Values.name}}
        image: {{.Values.image}}
        ports:
        - name: {{.Values.cluster_port_name}}
          containerPort: {{.Values.cluster_port}}
        - name: {{.Values.port_name}}
          containerPort: {{.Values.port}}
        resources:
          limits:
            cpu: {{.Values.data_cpu_limits}}
            memory: {{.Values.data_memory_limits}}
          requests:
            cpu: {{.Values.data_cpu_requests}}
            memory: {{.Values.data_memory_requests}}
        env:
        - name: CLUSTER_NAME
          value: "{{ .Values.cluster_name }}"
        - name: NODE_DATA
          value: "true"
        - name: NODE_MASTER
          value: "false"
        - name: SERVICE
          value: "{{ .Values.cluster_name }}"
        #  the min/max *must* be the same due to elasticsearch 5.0 bootstrap checks.  For a production settings
        #  this should be 16GB.  Don't go over 32GB as the JVM starts to have issues.
        - name: ES_JAVA_OPTS
          value: "-Xms{{ .Values.data_Xms_Xmx }} -Xmx{{ .Values.data_Xms_Xmx }} -Djava.net.preferIPv4Stack=true"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        imagePullPolicy: Always
      {{- if .Values.persistence.enabled }}
        volumeMounts:
        - name: esdata
          mountPath: /usr/share/elasticsearch/data
      {{- end }}
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
      {{- if .Values.data_tolerations }}
      tolerations:
      {{- range .Values.data_tolerations }}
      - key: {{ .key | default ("") }}
        value: {{ .value | default ("")}}
        operator: {{ .operator | default ("Equal") }}
        effect: {{ .effect  | default ("")}}
      {{- end }}
      {{- end }}
      {{- if .Values.data_scheduling.affinity }}
      {{- if .Values.data_scheduling.affinity.node }}
      {{- if .Values.data_scheduling.affinity.node.labels }}
      affinity:
        nodeAffinity:
          {{ .Values.data_scheduling.affinity.node.type }}:
            nodeSelectorTerms:
            - matchExpressions:
                {{- range .Values.data_scheduling.affinity.node.labels }}
                - key: {{ .key }}
                  operator: {{ .operator }}
                  values:
                  {{- range .values }}
                  - {{ . }}
                  {{- end }}
                {{- end }}
      {{- end }}
      {{- end }}
      {{- end }}
  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: esdata
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          #  set to small so when exploring with this the user doesn't accidentally create very large disks
          storage: {{.Values.data_volume_storage}}
  {{- end }}
